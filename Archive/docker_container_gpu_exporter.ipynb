{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26260506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prometheus_client \n",
    "from prometheus_client import start_http_server, Summary, Histogram, Info\n",
    "from prometheus_client.parser import text_string_to_metric_families \n",
    "from prometheus_client.core import GaugeMetricFamily, CounterMetricFamily, REGISTRY\n",
    "from prometheus_client import GC_COLLECTOR, PLATFORM_COLLECTOR, PROCESS_COLLECTOR\n",
    "import random\n",
    "import time\n",
    "import docker_gpu_exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dbf0d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTRY.unregister(GC_COLLECTOR)\n",
    "REGISTRY.unregister(PLATFORM_COLLECTOR)\n",
    "REGISTRY.unregister(PROCESS_COLLECTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39a53a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomCollector(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Run bash script\n",
    "        self.results_dict = {\"docker_container_running_gpu_pid\": 0,\n",
    "                \"docker_container_name\": \"\",\n",
    "                \"docker_container_used_gpu_id\": 0,\n",
    "                \"docker_container_utilization_gpu_percent\": 0,\n",
    "                \"docker_container_gpu_memory_used_MiB\": 0,\n",
    "                \"docker_container_total_gpu_used\": 0,\n",
    "               }\n",
    "        self.runnning_process = \"\"\"PID: 423900\n",
    "                            CONTAINER_NAME: ocr_containter_g03\n",
    "                            GPU util: 0 423900 96 44 3 423900 92 44\n",
    "                            GPU usage: 35383 MiB 35071 MiB\n",
    "\n",
    "\n",
    "                            PID: 1572603\n",
    "                            CONTAINER_NAME: kafka_container\n",
    "                            GPU util: 0 1572603 - - 1 1572603 - -\n",
    "                            GPU usage: 2307 MiB 7963 MiB\n",
    "\n",
    "\n",
    "                            PID: 377944\n",
    "                            CONTAINER_NAME: dev_container_v3\n",
    "                            GPU util: 1 377944 - -\n",
    "                            GPU usage: 8001 MiB\n",
    "\n",
    "\n",
    "                            PID: 2567679\n",
    "                            CONTAINER_NAME: nemo_container\n",
    "                            GPU util: 1 2567679 0 0\n",
    "                            GPU usage: 2771 MiB\n",
    "\n",
    "\n",
    "                            PID: 641061\n",
    "                            CONTAINER_NAME: containter_g3\n",
    "                            GPU util: 3 641061 - -\n",
    "                            GPU usage: 2193 MiB\n",
    "\n",
    "\n",
    "\n",
    "                            \"\"\"\n",
    "    def run_bash_script(self):\n",
    "        return docker_gpu_exporter.get_running_process()\n",
    "    \n",
    "        \n",
    "    def split_list(self, list_a, chunk_size):\n",
    "        segmented_list = []\n",
    "        for i in range(0, len(list_a), chunk_size):\n",
    "            segmented_list.append(list_a[i:i + chunk_size])\n",
    "        return segmented_list\n",
    "\n",
    "    def parse_bash_results(self, runnning_process):\n",
    "        for idx, container in enumerate(runnning_process.split(\"\\n\\n\")):\n",
    "            if (not (container)) or (len(\"\".join(container.split(\" \")))==0):\n",
    "                continue\n",
    "                \n",
    "            container_gpu_pid = container.split('PID: ')[1].split(\"\\n\")[0]\n",
    "            container_name = container.split('CONTAINER_NAME: ')[1].split(\"\\n\")[0]\n",
    "            container_gpu_util = container.split('GPU util: ')[1].split(\"\\n\")[0].split(' ')\n",
    "            container_gpu_usage = container.split('GPU usage: ')[1].split(\"\\n\")[0].split(' ')\n",
    "\n",
    "            if len(container_gpu_util) > 4:\n",
    "                container_gpu_util = self.split_list(container_gpu_util, 4)\n",
    "                container_gpu_usage = self.split_list(container_gpu_usage, 2)\n",
    "\n",
    "                container_gpu_ids = list(list(zip(*container_gpu_util))[0])\n",
    "                container_util_per_gpu = list(list(zip(*container_gpu_util))[3])\n",
    "                container_usage_per_gpu = list(list(zip(*container_gpu_usage))[0])\n",
    "                docker_container_total_gpu_used = len(container_gpu_util)\n",
    "            else:\n",
    "                container_gpu_ids = [container_gpu_util[0]]\n",
    "                container_util_per_gpu = [container_gpu_util[3]]\n",
    "                container_usage_per_gpu = [container_gpu_usage[0]]\n",
    "                docker_container_total_gpu_used = len(container_gpu_util)//4\n",
    "\n",
    "                \n",
    "            multi_gpu_result_list = []\n",
    "            for gpu_id, gpu_util, gpu_usage in zip(container_gpu_ids, container_util_per_gpu, container_usage_per_gpu):\n",
    "                print(\"container name: \", container_name)\n",
    "                metrics_resutls = self.results_dict.copy()\n",
    "                metrics_resutls[\"docker_container_running_gpu_pid\"] = container_gpu_pid\n",
    "                metrics_resutls[\"docker_container_name\"] = container_name\n",
    "\n",
    "                metrics_resutls[\"docker_container_used_gpu_id\"] = gpu_id\n",
    "                metrics_resutls[\"docker_container_utilization_gpu_percent\"] = \"0\" if gpu_util==\"-\" else gpu_util\n",
    "                metrics_resutls[\"docker_container_gpu_memory_used_MiB\"] = gpu_usage\n",
    "                metrics_resutls[\"docker_container_total_gpu_used\"] = str(docker_container_total_gpu_used)\n",
    "                multi_gpu_result_list.append(metrics_resutls)\n",
    "#                 print(multi_gpu_result_list)\n",
    "        return multi_gpu_result_list\n",
    "\n",
    "\n",
    "    def collect(self):\n",
    "        labels=[\"container_name\", \"gpu\"]\n",
    "        \n",
    "        results_dict_list = self.parse_bash_results(self.runnning_process)\n",
    "        \n",
    "#         result_dict = next(self.one_smaple_result_dict())\n",
    "        for result_dict in results_dict_list:\n",
    "            container_name = str(result_dict[\"docker_container_name\"])\n",
    "            gpu_id = str(result_dict[\"docker_container_used_gpu_id\"])\n",
    "\n",
    "            gauge_pid = GaugeMetricFamily('docker_container_running_gpu_pid', 'What pid is the gpu container', labels=labels)\n",
    "            gauge_pid.add_metric([container_name, gpu_id], value=result_dict['docker_container_running_gpu_pid'])\n",
    "            yield gauge_pid\n",
    "\n",
    "            gauge_name =  GaugeMetricFamily('docker_container_name', 'Container name', labels=labels)\n",
    "            gauge_name.add_metric([container_name, gpu_id], value=1)\n",
    "            yield gauge_name\n",
    "\n",
    "            gauge_gpu_id =  GaugeMetricFamily('docker_container_used_gpu_id', 'Container used gpu', labels=labels)\n",
    "            gauge_name.add_metric([container_name, gpu_id], value=result_dict['docker_container_used_gpu_id'])\n",
    "            yield gauge_name\n",
    "\n",
    "            gauge_util = GaugeMetricFamily('docker_container_utilization_gpu_percent', 'Help text', labels=labels)\n",
    "            gauge_util.add_metric([container_name, gpu_id], value=result_dict['docker_container_utilization_gpu_percent'])\n",
    "            yield gauge_util\n",
    "\n",
    "            gauge_usage = GaugeMetricFamily('docker_container_gpu_memory_used_MiB', 'Help text', labels=labels)\n",
    "            gauge_usage.add_metric([container_name, gpu_id], value=result_dict['docker_container_gpu_memory_used_MiB'])\n",
    "            yield gauge_usage\n",
    "\n",
    "            counter_gpu = CounterMetricFamily('docker_container_total_gpu_used', 'Help text', labels=labels)\n",
    "            counter_gpu.add_metric([container_name, gpu_id], value=result_dict['docker_container_total_gpu_used'])\n",
    "            yield counter_gpu\n",
    "REGISTRY.register(CustomCollector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "for family in text_string_to_metric_families():\n",
    "    for sample in family.samples:\n",
    "        print(\"{0}{1} {2}\".format(*sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4109a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    port = 10046\n",
    "    start_http_server(port)\n",
    "#     REGISTRY.register(CustomCollector())\n",
    "    while True:\n",
    "#         # period between collection\n",
    "        time.sleep(10)\n",
    "#         break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f9bc678a26cd82d808fde2eb1f8bff70d5d21702cf1d624c7a65a12be14e27c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
