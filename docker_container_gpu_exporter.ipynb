{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26260506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prometheus_client \n",
    "from prometheus_client import start_http_server, Summary, Histogram, Info\n",
    "from prometheus_client.parser import text_string_to_metric_families \n",
    "from prometheus_client.core import GaugeMetricFamily, CounterMetricFamily, REGISTRY\n",
    "from prometheus_client import GC_COLLECTOR, PLATFORM_COLLECTOR, PROCESS_COLLECTOR\n",
    "import random\n",
    "import time\n",
    "import docker_gpu_exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dbf0d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTRY.unregister(GC_COLLECTOR)\n",
    "REGISTRY.unregister(PLATFORM_COLLECTOR)\n",
    "REGISTRY.unregister(PROCESS_COLLECTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39a53a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container name:  ocr_containter_g03\n",
      "container name:  ocr_containter_g03\n",
      "container name:  kafka_spaces_asr_diarization_container\n",
      "container name:  kafka_spaces_asr_diarization_container\n",
      "container name:  asr_sp_arsa_dev_faris_v3\n",
      "container name:  nemo_faris3\n",
      "container name:  mfdr_containter_g3\n"
     ]
    }
   ],
   "source": [
    "class CustomCollector(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Run bash script\n",
    "        self.results_dict = {\"docker_container_running_gpu_pid\": 0,\n",
    "                \"docker_container_name\": \"\",\n",
    "                \"docker_container_used_gpu_id\": 0,\n",
    "                \"docker_container_utilization_gpu_percent\": 0,\n",
    "                \"docker_container_gpu_memory_used_MiB\": 0,\n",
    "                \"docker_container_total_gpu_used\": 0,\n",
    "               }\n",
    "        self.runnning_process = \"\"\"PID: 423900\n",
    "                            CONTAINER_NAME: ocr_containter_g03\n",
    "                            GPU util: 0 423900 96 44 3 423900 92 44\n",
    "                            GPU usage: 35383 MiB 35071 MiB\n",
    "\n",
    "\n",
    "                            PID: 1572603\n",
    "                            CONTAINER_NAME: kafka_spaces_asr_diarization_container\n",
    "                            GPU util: 0 1572603 - - 1 1572603 - -\n",
    "                            GPU usage: 2307 MiB 7963 MiB\n",
    "\n",
    "\n",
    "                            PID: 377944\n",
    "                            CONTAINER_NAME: asr_sp_arsa_dev_faris_v3\n",
    "                            GPU util: 1 377944 - -\n",
    "                            GPU usage: 8001 MiB\n",
    "\n",
    "\n",
    "                            PID: 2567679\n",
    "                            CONTAINER_NAME: nemo_faris3\n",
    "                            GPU util: 1 2567679 0 0\n",
    "                            GPU usage: 2771 MiB\n",
    "\n",
    "\n",
    "                            PID: 641061\n",
    "                            CONTAINER_NAME: mfdr_containter_g3\n",
    "                            GPU util: 3 641061 - -\n",
    "                            GPU usage: 2193 MiB\n",
    "\n",
    "\n",
    "\n",
    "                            \"\"\"\n",
    "    def run_bash_script(self):\n",
    "        return docker_gpu_exporter.get_running_process()\n",
    "    \n",
    "        \n",
    "    def split_list(self, list_a, chunk_size):\n",
    "        segmented_list = []\n",
    "        for i in range(0, len(list_a), chunk_size):\n",
    "            segmented_list.append(list_a[i:i + chunk_size])\n",
    "        return segmented_list\n",
    "\n",
    "    def parse_bash_results(self, runnning_process):\n",
    "        for idx, container in enumerate(runnning_process.split(\"\\n\\n\")):\n",
    "            if (not (container)) or (len(\"\".join(container.split(\" \")))==0):\n",
    "                continue\n",
    "                \n",
    "            container_gpu_pid = container.split('PID: ')[1].split(\"\\n\")[0]\n",
    "            container_name = container.split('CONTAINER_NAME: ')[1].split(\"\\n\")[0]\n",
    "            container_gpu_util = container.split('GPU util: ')[1].split(\"\\n\")[0].split(' ')\n",
    "            container_gpu_usage = container.split('GPU usage: ')[1].split(\"\\n\")[0].split(' ')\n",
    "\n",
    "            if len(container_gpu_util) > 4:\n",
    "                container_gpu_util = self.split_list(container_gpu_util, 4)\n",
    "                container_gpu_usage = self.split_list(container_gpu_usage, 2)\n",
    "\n",
    "                container_gpu_ids = list(list(zip(*container_gpu_util))[0])\n",
    "                container_util_per_gpu = list(list(zip(*container_gpu_util))[3])\n",
    "                container_usage_per_gpu = list(list(zip(*container_gpu_usage))[0])\n",
    "                docker_container_total_gpu_used = len(container_gpu_util)\n",
    "            else:\n",
    "                container_gpu_ids = [container_gpu_util[0]]\n",
    "                container_util_per_gpu = [container_gpu_util[3]]\n",
    "                container_usage_per_gpu = [container_gpu_usage[0]]\n",
    "                docker_container_total_gpu_used = len(container_gpu_util)//4\n",
    "\n",
    "                \n",
    "            multi_gpu_result_list = []\n",
    "            for gpu_id, gpu_util, gpu_usage in zip(container_gpu_ids, container_util_per_gpu, container_usage_per_gpu):\n",
    "                print(\"container name: \", container_name)\n",
    "                metrics_resutls = self.results_dict.copy()\n",
    "                metrics_resutls[\"docker_container_running_gpu_pid\"] = container_gpu_pid\n",
    "                metrics_resutls[\"docker_container_name\"] = container_name\n",
    "\n",
    "                metrics_resutls[\"docker_container_used_gpu_id\"] = gpu_id\n",
    "                metrics_resutls[\"docker_container_utilization_gpu_percent\"] = \"0\" if gpu_util==\"-\" else gpu_util\n",
    "                metrics_resutls[\"docker_container_gpu_memory_used_MiB\"] = gpu_usage\n",
    "                metrics_resutls[\"docker_container_total_gpu_used\"] = str(docker_container_total_gpu_used)\n",
    "                multi_gpu_result_list.append(metrics_resutls)\n",
    "#                 print(multi_gpu_result_list)\n",
    "        return multi_gpu_result_list\n",
    "\n",
    "\n",
    "    def collect(self):\n",
    "        labels=[\"container_name\", \"gpu\"]\n",
    "        \n",
    "        results_dict_list = self.parse_bash_results(self.runnning_process)\n",
    "        \n",
    "#         result_dict = next(self.one_smaple_result_dict())\n",
    "        for result_dict in results_dict_list:\n",
    "            container_name = str(result_dict[\"docker_container_name\"])\n",
    "            gpu_id = str(result_dict[\"docker_container_used_gpu_id\"])\n",
    "\n",
    "            gauge_pid = GaugeMetricFamily('docker_container_running_gpu_pid', 'What pid is the gpu container', labels=labels)\n",
    "            gauge_pid.add_metric([container_name, gpu_id], value=result_dict['docker_container_running_gpu_pid'])\n",
    "            yield gauge_pid\n",
    "\n",
    "            gauge_name =  GaugeMetricFamily('docker_container_name', 'Container name', labels=labels)\n",
    "            gauge_name.add_metric([container_name, gpu_id], value=1)\n",
    "            yield gauge_name\n",
    "\n",
    "            gauge_gpu_id =  GaugeMetricFamily('docker_container_used_gpu_id', 'Container used gpu', labels=labels)\n",
    "            gauge_name.add_metric([container_name, gpu_id], value=result_dict['docker_container_used_gpu_id'])\n",
    "            yield gauge_name\n",
    "\n",
    "            gauge_util = GaugeMetricFamily('docker_container_utilization_gpu_percent', 'Help text', labels=labels)\n",
    "            gauge_util.add_metric([container_name, gpu_id], value=result_dict['docker_container_utilization_gpu_percent'])\n",
    "            yield gauge_util\n",
    "\n",
    "            gauge_usage = GaugeMetricFamily('docker_container_gpu_memory_used_MiB', 'Help text', labels=labels)\n",
    "            gauge_usage.add_metric([container_name, gpu_id], value=result_dict['docker_container_gpu_memory_used_MiB'])\n",
    "            yield gauge_usage\n",
    "\n",
    "            counter_gpu = CounterMetricFamily('docker_container_total_gpu_used', 'Help text', labels=labels)\n",
    "            counter_gpu.add_metric([container_name, gpu_id], value=result_dict['docker_container_total_gpu_used'])\n",
    "            yield counter_gpu\n",
    "REGISTRY.register(CustomCollector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "for family in text_string_to_metric_families():\n",
    "    for sample in family.samples:\n",
    "        print(\"{0}{1} {2}\".format(*sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4109a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    port = 10046\n",
    "    start_http_server(port)\n",
    "#     REGISTRY.register(CustomCollector())\n",
    "    while True:\n",
    "#         # period between collection\n",
    "        time.sleep(10)\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eda338",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7974800",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"docker_container_running_gpu_pid\": 0,\n",
    "                \"docker_container_name\": \"\",\n",
    "                \"docker_container_used_gpu_id\": 0,\n",
    "                \"docker_container_utilization_gpu_percent\": 0,\n",
    "                \"docker_container_gpu_memory_used_MiB\": 0,\n",
    "                \"docker_container_total_gpu_used\": 0,\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df64665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTRY.register(CustomCollector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc47adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metric to track time spent and requests made.\n",
    "\n",
    "REQUEST_TIME = prometheus_client.generate_latest() #Summary('Nawaf', 'test')\n",
    "\n",
    "# Decorate function with metric.\n",
    "@REQUEST_TIME.time()\n",
    "def process_request(t):\n",
    "    \"\"\"A dummy function that takes some time.\"\"\"\n",
    "    time.sleep(t)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Start up the server to expose the metrics.\n",
    "    start_http_server(10055)\n",
    "    # Generate some requests.\n",
    "    while True:\n",
    "        process_request(random.random())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff22d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efe301c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987788fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(REGISTRY.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(REGISTRY.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary('my_gauge', 'Help text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ff037",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = Histogram('request_latency_seconds', 'Description of histogram')\n",
    "h.observe(4.7, {'trace_id': 'abc123'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "value1 = GaugeMetricFamily(\"SERVER_STATUS\", 'Help text', labels='value')\n",
    "value1.add_metric([\"cpu_usage\"], cpu_usage)\n",
    "yield value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    start_http_server(10055)         ## port where metrics need to be exposed.\n",
    "    REGISTRY.register(CustomCollector())\n",
    "    while True:\n",
    "        time.sleep(10)\t\t       ## To collect the metrics for every 30s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623f7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker_gpu_exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e8a7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h = docker_gpu_exporter.get_runnning_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a460e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnning_process = \"\"\"PID: 423900\n",
    "CONTAINER_NAME: ocr_containter_g03\n",
    "GPU util: 0 423900 96 44 3 423900 92 44\n",
    "GPU usage: 35383 MiB 35071 MiB\n",
    "\n",
    "\n",
    "PID: 1572603\n",
    "CONTAINER_NAME: kafka_spaces_asr_diarization_container\n",
    "GPU util: 0 1572603 - - 1 1572603 - -\n",
    "GPU usage: 2307 MiB 7963 MiB\n",
    "\n",
    "\n",
    "PID: 377944\n",
    "CONTAINER_NAME: asr_sp_arsa_dev_faris_v3\n",
    "GPU util: 1 377944 - -\n",
    "GPU usage: 8001 MiB\n",
    "\n",
    "\n",
    "PID: 2567679\n",
    "CONTAINER_NAME: nemo_faris3\n",
    "GPU util: 1 2567679 0 0\n",
    "GPU usage: 2771 MiB\n",
    "\n",
    "\n",
    "PID: 641061\n",
    "CONTAINER_NAME: mfdr_containter_g3\n",
    "GPU util: 3 641061 - -\n",
    "GPU usage: 2193 MiB\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e83700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(list_a, chunk_size):\n",
    "    segmented_list = []\n",
    "    for i in range(0, len(list_a), chunk_size):\n",
    "        segmented_list.append(list_a[i:i + chunk_size])\n",
    "    return segmented_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c50ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\"docker_container_running_gpu_pid\": 0,\n",
    "                \"docker_container_name\": \"\",\n",
    "                \"docker_container_used_gpu_id\": 0,\n",
    "                \"docker_container_utilization_gpu_percent\": 0,\n",
    "                \"docker_container_gpu_memory_used_MiB\": 0,\n",
    "                \"docker_container_total_gpu_used\": 0,\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796552aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for container in runnning_process.split(\"\\n\\n\"):\n",
    "    if not (container):\n",
    "        continue\n",
    "    container_gpu_pid = container.split('PID: ')[1].split(\"\\n\")[0]\n",
    "    container_name = container.split('CONTAINER_NAME: ')[1].split(\"\\n\")[0]\n",
    "    container_gpu_util = container.split('GPU util: ')[1].split(\"\\n\")[0].split(' ')\n",
    "    container_gpu_usage = container.split('GPU usage: ')[1].split(\"\\n\")[0].split(' ')\n",
    "    \n",
    "    multi_gpu = False\n",
    "    if len(container_gpu_util) > 4:\n",
    "        container_gpu_util = split_list(container_gpu_util, 4)\n",
    "        container_gpu_usage = split_list(container_gpu_usage, 2)\n",
    "        \n",
    "        container_gpu_ids = list(list(zip(*container_gpu_util))[0])\n",
    "        container_util_per_gpu = list(list(zip(*container_gpu_util))[3])\n",
    "        container_usage_per_gpu = list(list(zip(*container_gpu_usage))[0])\n",
    "        docker_container_total_gpu_used = len(container_gpu_util)\n",
    "        multi_gpu=True\n",
    "    else:\n",
    "        container_gpu_ids = container_gpu_util[0]\n",
    "        container_util_per_gpu = container_gpu_util[3]\n",
    "        container_usage_per_gpu = container_gpu_usage[0]\n",
    "        docker_container_total_gpu_used = len(container_gpu_util)//4\n",
    "    \n",
    "    if multi_gpu:\n",
    "        for gpu_id, gpu_util, gpu_usage in zip(container_gpu_ids, container_util_per_gpu, container_usage_per_gpu):\n",
    "            metrics_resutls = results_dict.copy()\n",
    "            metrics_resutls[\"docker_container_running_gpu_pid\"] = gpu_id\n",
    "            metrics_resutls[\"docker_container_name\"] = container_name\n",
    "\n",
    "            metrics_resutls[\"docker_container_used_gpu_id\"] = gpu_id\n",
    "            metrics_resutls[\"docker_container_utilization_gpu_percent\"] = gpu_util\n",
    "            metrics_resutls[\"docker_container_gpu_memory_used_MiB\"] = gpu_usage\n",
    "            metrics_resutls[\"docker_container_total_gpu_used\"] = docker_container_total_gpu_used\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab4e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180df25e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "container_gpu_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_gpu_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_gpu_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_util_per_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116ece27",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_gpu_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abcb3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_gpu_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e763b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_usage_per_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_container_total_gpu_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_smaple_result_dict(my_list = [1, 2, 3, 4, 5]):\n",
    "    \n",
    "    #bash_script_results = self.run_bash_script()\n",
    "    for i in my_list:\n",
    "\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a30661",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(one_smaple_result_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in one_smaple_result_dict():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec44f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be7636fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: docker: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!docker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
